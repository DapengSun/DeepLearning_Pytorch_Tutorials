# -*- coding: utf-8 -*-import numpy as npimport random###使用numpy实现的MLP思路:# 输入层=28*28=784# hideen layer=30# output layer=10# @ == mamul# * == element-wise# w1 = [b, 784] @ [784, 30] = [b, 30]def sigmoid(z):    return 1.0/(1.0+np.exp(-z))def dsigmoid_prime(z):    return sigmoid(z)(1-sigmoid(z))class MLP_np():    def __init__(self, sizes):        """        :param sizes: [784, 30, 10]        """        self.sizes = sizes        self.num_layers = len(sizes)-1        # sizes:[784, 30, 10]        # w:[ch_out, ch_in]        # zip(sizes[:-1] = [784,30], sizes[1:] = [30,10])        self.weights = [np.random.randn(ch2, ch1) for ch1, ch2 in zip(sizes[:-1], sizes[1:])]        # bias:[ch_out]        # z = wx + b => b:[30,1]        self.biases = [np.random.randn(ch, 1) for ch in sizes[1:]]    def forward(self, x):        """        :param x: [784, 1]        :return: [10, 1]        """        for b, w in zip(self.biases, self.weights):#zip将数据打包            # w:[30, 784]@[784, 1] => [30, 1]+[30, 1] => [30, 1]            z = np.dot(w, x) + b #矩阵相乘            # [30, 1]            x = sigmoid(z)        return x    def backprop(self, x, y):        """        :param x: [784, 1]        :param y: [10, 1], one_hot encoding        :return:        nabla:倒三角        """        nabla_w = [np.zeros(w.shape) for w in self.weights]        nabla_b = [np.zeros(b.shape) for b in self.biases]        # 1.forward        # save activation for every layer        activations = [x]        # save z for every size        zs = []        activation = x        for b, w in zip(self.biases, self.weights):            z = np.dot(w, activation) + b            activation = sigmoid(z)            zs.append(z)            activations.append(activation)        loss = np.power(activations[-1] - y, 2).sum()        # 2.backward        # 2.1 compute gradient on output layer        # [10, 1] with [10, 1] => [10, 1]        delta = activation[-1] * (1 - activations[-1]) * (activations[-1] - y)# delta_k        nabla_b[-1] = delta        # [10, 1] @ [1, 30] => [10, 30]        # activation[-2] == [30,1]        nabla_w[-1] = np.dot(delta, activations[-2].T)        # 2.2 compute hidden gradient        for l in range(2, self.num_layers+1):            l = -l            z = zs[l]            a = activations[l]            # delta_j            # [10,30]T@[10, 1] => [30, 10]@[10, 1]=>[30,1]*[30,1]=>[30,1]            delta = np.dot(self.weights[l+1].T, delta) * a * (1 - a)#np.dot()是因为求和            nabla_b[l] = delta            # [30, 1] @ [784, 1]T = [30, 784]            nabla_w[l] = np.dot(delta, activations[l-1].T)#数据格式不一样        return nabla_w, nabla_b, loss    def train(self, training_data, epochs, batchsz, lr, test_data):        """        :param training_data: list of (x, y)        :param epochs: 1000        :param batchsz: 1        :param lr: 0.01        :param test_data: list of (x, y)        :return:        """        if test_data:            n_test = len(test_data)        n = len(training_data)        for j in range(epochs):            random.shuffle(training_data)            mini_batches = [                training_data[k:k+batchsz]                for k in range(0, n, batchsz)#一次按batchsize取            ]            # for every batch  in current batch            for mini_batch in mini_batches:                loss = self.update_mini_batch(mini_batch, lr)            if test_data:                print("Epoch {0}: {1} / {2}".format(j, self.evluate(test_data), n_test), loss)            else:                print("Epoch {0} complete".format(j))    def update_mini_batch(self, batch, lr):        """        :param mini_batch: list of (x,y)        :param lr: 0.01        :return:        """        # https://en.wikipedia.org/wiki/Del        nabla_b = [np.zeros(b.shape) for b in self.biases]        nabla_w = [np.zeros(w.shape) for w in self.weights]        loss = 0        # for every sample  in current batch        for x, y in batch:            # list of every w/b gradient            # [w1, w2, w3]            nabla_w_, nabla_b_, loss_ = self.backprop(x, y)            nabla_w = [accu+cur for accu, cur in zip(nabla_w, nabla_w_)]#实现对应位置的累加            nabla_b = [accu+cur for accu, cur in zip(nabla_b, nabla_b_)]            loss += loss_        # tmp1 = [np.linalg.norm(b/len(mini_batch)) for b in nabla_b]        # tmp2 = [np.linalg.norm(w/len(mini_batch)) for w in nabla_w]        # print(tmp1)        # print(tmp2)        nabla_w = [w/len(batch) for w in nabla_w]        nabla_b = [b/len(batch) for b in nabla_b]        # w = w - lr * nabla_w        self.weights = [w - lr * nabla for w, nabla in zip(self.weights, nabla_w)]        self.biases = [b - lr * nabla for b, nabla in zip(self.biases, nabla_b)]        loss = loss / len(batch)# batch的平均loss        return loss    def evluate(self, test_data):        """        y is not hot coding.标量        :param test_data: list of (x, y)        :return:        """        test_results = [(np.argmax(self.forward(x)), y)                        for (x, y) in test_data]        correct = sum(int(pred == y) for (pred, y) in test_results)#对的元素        return correctdef main():    import mnist_loader    # Loading the MNIST data    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()    # (50000, 784) (50000, 10) (10000, 784) (10000, 10)    print(len(training_data), training_data[0][0].shape, training_data[0][1].shape)#50000 (784, 1) (10, 1)    print(len(test_data), test_data[0][0].shape, test_data[0][1].shape)#10000 (784, 1) ()    print(test_data[0][1])    np.random.seed(66)    # Set up a Network with 30 hidden neurous    net = MLP_np([784, 30, 10])    # data_train = list(zip(x_train, y_train))    # data_test = list(zip(x_test, y_test))    net.train(training_data, 10000, 10, 0.001, test_data=test_data)if __name__ == '__main__':    main()